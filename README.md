## Toward User-Aware Interactive Virtual Agents: Generative Multi-Modal<br> Agent Behaviors in VR

![Teaser](https://github.com/user-attachments/assets/4a66fddc-3107-4a74-947b-680b0c8d460d)

### Abstract 
Virtual agents serve as a vital interface within XR platforms. How-
ever, generating virtual agent behaviors typically rely on pre-coded
actions or physics-based reactions. In this paper we present a
learning-based multimodal agent behavior generation framework
that adapts to users’ in-situ behaviors, similar to how humans in-
teract with each other in the real world. By leveraging an in-house
collected, dyadic conversational behavior dataset, we trained a con-
ditional variational autoencoder (CVAE) model to achieve user-
conditioned generation of virtual agents’ behaviors. Together with
large language models (LLM), our approach can generate both the
verbal and non-verbal reactive behaviors of virtual agents. Our
comparative user study confirmed our method’s superiority over con-
ventional animation graph-based baseline techniques, particularly
regarding user-centric criteria. Thorough analyses of our results
underscored the authentic nature of our virtual agents’ interactions
and the heightened user engagement during VR interaction.

[Source](https://github.com/cg-im/ismar24_intelligent_agent)

Fill [this form](https://forms.gle/SoRcmLBgWxQqiyoo8) to request the data.
 
